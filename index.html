<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xuyang Bai</title>
  
  <meta name="author" content="Xuyang Bai">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xuyang BAI 白旭阳</name>
              </p>
              <p> I am a fourth-year PhD student (2018-) of Computer Science and Engineering at <a href="https://www.ust.hk/">HKUST</a>, advised by <a href="http://www.cse.ust.hk/~taicl/">Prof. Chiew-Lan Tai</a> at <a href="http://visgraph.cse.ust.hk/">Vision and Graphics Group</a>. 
                I am also working closely with <a href="https://www.cse.ust.hk/~quan/">Prof. Long Quan</a> and <a href="http://sweb.cityu.edu.hk/hongbofu/">Prof. Hongbo Fu</a>. Prior to HKUST, I received my BSc in Electronic Engenieering from Beijing Normal University. </p>
              <p style="text-align:center">
                <a href="mailto:xbaiad@connect.ust.hk">Email</a> &nbsp/&nbsp
                <a href="data/CV_XuyangBAI.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=UL2svm4AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/Xuyang74483763">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/XuyangBai/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/XuyangBAI.jpg"><img style="width:70%;max-width:100%" alt="profile photo" src="images/XuyangBAI.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
              <tr>
                  <td style="padding:20px;width:100%;vertical-align:middle">
                      <heading>News</heading>
                      <p></p>
                      <li>2021.11: Our work TransFusion outperforms all the non-ensembled methods in the leaderboard of  <a href="https://www.nuscenes.org/object-detection">nuScenes detection</a> and achieves the 1st place in the leaderboard of <a href="https://www.nuscenes.org/tracking">nuScenes tracking</a> on open track. </li>
                  </td>
              </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in 3D computer vision, machine learning, computer graphics. Specifically, I did some works about building correspondences (for point clouds or images) and scene understanding (detection or segmentation). Free free to drop me an e-mail if you are insterested in my work.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px ;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/TransFusion_CVPR2022.png" alt="SGMNet" width="200" height="" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2203.11496.pdf">
                <papertitle>TransFusion: Robust LiDAR-Camera Fusion for 3D Object Detection with Transformers.</papertitle>
              </a>
              <br>
              <strong>Xuyang Bai</strong>, Zeyu Hu, Xinge Zhu, Qingqiu Huang, Yilun Chen, Hongbo Fu, Chiew-Lan Tai
              <br>
              <em>CVPR</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2203.11496.pdf">paper</a> / 
              <a href="https://github.com/XuyangBai/TransFusion">code</a> / 
              <a href="data/TransFusion_CVPR2021.bib">bibtex</a>
              <!-- <p>A computational efficient image feature matching model which adopts a graph neural network with sparse structure to reduce redundant connectivity and learn compact representation. </p> -->
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/SGMNet_ICCV2021.png" alt="SGMNet" width="200" height="" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2108.08771.pdf">
                <papertitle>Learning to Match Features with Seeded Graph Matching Network.</papertitle>
              </a>
              <br>
              Hongkai Chen, Zixin Luo, Jiahui Zhang, Lei Zhou, <strong>Xuyang Bai</strong>, Zeyu Hu, Chiew-Lan Tai, Long Quan
              <br>
              <em>ICCV</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2108.08771.pdf">paper</a> / 
              <a href="https://github.com/vdvchen/SGMNet">code</a> / 
              <a href="data/SGMNet_ICCV2021.bib">bibtex</a>
              <!-- <p>A computational efficient image feature matching model which adopts a graph neural network with sparse structure to reduce redundant connectivity and learn compact representation. </p> -->
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/VMNet_ICCV2021.png" alt="VMNet" width="200" height="">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2107.13824">
                <papertitle>VMNet: Voxel-Mesh Network for Geodesic-Aware 3D Semantic Segmentation</papertitle>
              </a>
              <br>
              Zeyu Hu, <strong>Xuyang Bai</strong>, Jiaxiang Shang, Runze Zhang, Jiayu Dong, Xin Wang, Guangyuan Sun, Hongbo Fu, Chiew-Lan Tai
              <br>
              <em>ICCV</em>, 2021 <a style="color:#FF0000";>(Oral)</a>
              <br>
              <a href="https://arxiv.org/pdf/2107.13824">paper</a> / 
              <a href="https://github.com/hzykent/VMNet">code</a> / 
              <a href="data/VMNet_ICCV2021.bib">bibtex</a>
              <!-- <p>A 3D deep architecture for semantic segmentation that operates on the voxel and mesh representations leveraging both the Euclidean and geodesic information. -->
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/PointDSC_CVPR2021.png" alt="SGMNet" width="200" height="" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2103.05465">
                <papertitle>PointDSC: Robust Point Cloud Registration using Deep Spatial Consistency.</papertitle>
              </a>
              <br>
              <strong>Xuyang Bai</strong>, Zixin Luo, Lei Zhou, Hongkai Chen, Lei Li, Zeyu Hu, Hongbo Fu, Chiew-Lan Tai
              <br>
              <em>CVPR</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2103.05465.pdf">paper</a> / 
              <a href="https://github.com/XuyangBai/PointDSC">code</a> / 
              <a href="data/PointDSC_CVPR2021.bib">bibtex</a>
              <!-- <p>A robust outlier rejection model for point cloud registration that explicitly leverages the spatial consistency established by Euclidean transformations between point clouds. </p> -->
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/JSENet_ECCV2020.png" alt="SGMNet" width="200" height="" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2007.06888">
                <papertitle>JSENet: Joint Semantic Segmentation and Edge Detection Network for 3D Point Clouds.</papertitle>
              </a>
              <br>
              Zeyu Hu, Mingmin Zhen, <strong>Xuyang Bai</strong>, Hongbo Fu, Chiew-lan Tai
              <br>
              <em>ECCV</em>, 2020
              <br>
              <a href="https://arxiv.org/pdf/2007.06888.pdf">paper</a> / 
              <a href="https://github.com/hzykent/JSENet">code</a> / 
              <a href="data/JSENet_ECCV2020.bib">bibtex</a>
              <!-- <p>. </p> -->
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ASLFeat_CVPR2020.png" alt="SGMNet" width="200" height="" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2003.10071">
                <papertitle>ASLFeat: Learning Local Features of Accurate Shape and Localization.</papertitle>
              </a>
              <br>
              Zixin Luo, Lei Zhou, <strong>Xuyang Bai</strong>, Hongkai Chen, Jiahui Zhang, Yao Yao, Shiwei Li, Tian Fang, Long Quan
              <br>
              <em>CVPR</em>, 2020
              <br>
              <a href="https://arxiv.org/pdf/2003.10071.pdf">paper</a> / 
              <a href="https://github.com/lzx551402/ASLFeat">code</a> / 
              <a href="data/ASLFeat_CVPR2020.bib">bibtex</a>
              <!-- <p> </p> -->
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/D3Feat_CVPR2020.png" alt="SGMNet" width="200" height="" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2003.03164">
                <papertitle>D3Feat: Joint Learning of Dense Detection and Description of 3D Local Features.</papertitle>
              </a>
              <br>
              <strong>Xuyang Bai</strong>, Zixin Luo, Lei Zhou, Hongbo Fu, Long Quan, Chiew-Lan Tai
              <br>
              <em>CVPR</em>, 2020 <a style="color:#FF0000";>(Oral)</a>
              <br>
              <a href="https://arxiv.org/pdf/2003.03164">paper</a> / 
              <a href="https://github.com/XuyangBai/D3Feat">code</a> / 
              <a href="data/D3Feat_CVPR2020.bib">bibtex</a>
              <!-- <p>A joint learning framework for 3D local feature detection and description. </p> -->
            </td>
          </tr>


        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
              <tr>
                  <td style="padding:20px;width:100%;vertical-align:middle">
                      <heading>Experience</heading>
                      <p></p>
                      <li>Huawei Intelligent Automotive Solution BU, Mar.2021-Present</li>
                      <li>Megvii Research ShangHai, Sept.2020-Feb.2021</li>
                  </td>
              </tr>
          </tbody>
      </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <heading>Reviewer Services</heading>
                        <p></p>
                        <li>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</li>
                        <li>Computers & Graphics</li>
                    </td>
                </tr>
            </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Last update: Mar. 2022. Thanks for the <a href="https://jonbarron.info/">template</a> of Jon Barron.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
